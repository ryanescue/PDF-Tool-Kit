Post-MVP Peer Code Review Project Reviewed: PhishHawk / Email Scanner Reviewer: Sean Angrisani Author: Ryan Escue Date: 12/8/2025 1. Project Understanding a. What is the objective of the project? This project is a phishing email scanner. Its main goal is to help users determine whether an email might be a scam. The basic flow is: ● The user pastes an email’s text (and optionally the raw headers) into a webpage. ● The app runs through several checks on that text and header info. ● It returns: ○ A score from 0–100 (higher scores mean the email seems more suspicious), ○ A verdict like “Likely Safe,” “Potentially Phishing,” or “High Risk,” ○ A breakdown that lists which rules fired and why the score is what it is. There is also a Google Apps Script that scans Gmail for emails with a specific label (forwarded) and sends information about those emails into a Google Sheet. That is used to collect real examples of emails for testing or analysis. Overall, the project is trying to make it easier for everyday users to spot phishing emails and learn what kinds of patterns are red flags. b. Technologies used From reviewing the repository, the project uses: ● Frontend (what runs in the browser): ○ HTML for the basic page structure. ○ CSS (style.css) for layout and styling (cards, buttons, score circle, etc.). ○ Vanilla JavaScript (app.js) to: ■ Read user input, ■ Run phishing checks, ■ Update the score, verdict, and explanation on the page. ● Firebase: ○ Firebase Hosting to host the static site from the public/ folder. ○ A Firebase Functions project in the functions/ folder, written in TypeScript. Right now, it mostly has boilerplate—global options and a commented-out example—no real app logic yet. ○ Cloud Firestore is configured, but obviously not used in the current app code. ● Google Apps Script: ○ Uses Gmail App to read email threads with the label forwarded. ○ Uses Spreadsheet App to write data into a Google Sheet. ○ Has a helper function parseForwardedBlock that tries to pull the “original sender” and original body out of a forwarded message. In short, the main phishing analysis happens in the browser, while Apps Script is used as a helper tool to pull real emails into a sheet. 2. Problem-Solving a. Most significant challenge The biggest challenge in this project seems to be taking messy email content and turning it into a useful “this looks safe / this looks sketchy” score. There are two main parts to that: 1. Parsing forwarded emails in the Apps Script ○ The script must deal with different ways forwarded messages might be formatted in Gmail. ○ It tries to figure out who the original message was from and where the original body starts, even if the text isn’t perfectly structured. 2. Building a scoring system in app.js ○ The analyzer must look at text and headers and decide which patterns are suspicious (for example, certain kinds of links or certain keywords). ○ Then it must turn all those signals into one combined score and verdict. Both are harder than they look at first, because real emails are inconsistent and not always “clean.” b. How the student explained their approach The student explained their approach mainly through the code itself: ● Clear naming: Functions like extractUrls, hasPunycodeOrUnicode, findDisplayHrefMismatches, and parseHeaderDomain are self-explanatory. DOM variables such as scoreCircle, scoreBar, and breakdownList also make the UI logic easy to follow. ● Comments: The Apps Script includes comments explaining the kind of forwarded message format it expects and what parseForwardedBlock returns. There are comments indicating optional behavior (like whether to remove Gmail labels after processing, or that Firebase was “removed at the moment”). ● Project setup: The presence of Firebase configuration, emulators, and build scripts shows there was thought given to deployment and future backend work, even if it isn’t fully implemented yet. Even without a big design document, the code is organized clearly enough that the overall approach is understandable. c. Creativity and unique decisions There are several creative touches: ● Multiple realistic phishing checks: The analyzer doesn’t just look for one or two keywords. It checks for: ○ Link shorteners (bit.ly, tinyurl, t.co, etc.), ○ Links that use raw IP addresses instead of domains, ○ Insecure http:// links, ○ Punycode or unusual characters in domains, ○ “Urgent” or threatening language, ○ Login/password/billing requests, ○ Suspicious attachment extensions mentioned in the email text, ○ Mismatches between From and Reply-To domains in headers. ● Detecting mismatch between visible link text and actual URL: The findDisplayHrefMismatches function looks at markdown-style and HTML links and flags cases where the displayed text suggests one domain but the link actually goes somewhere else. That’s a classic phishing behavior and a smart thing to look for. ● Explaining the score to the user: The breakdown list that shows which rules matched (e.g., “Shortener: bit.ly (+15)”) helps users understand why the email was flagged. That makes the tool educational, not just a black box. ● Using Gmail labels and Sheets: Using a Gmail label based workflow (forwarded) and sending results to a sheet is simple but effective. It’s a practical way to collect data without building a full server. 3. Code Quality a. Readability and organization Front-end (app.js): ● The file is structured logically: ○ DOM elements are grabbed at the top. ○ Helper functions come next. ○ The main analyzeBtn click handler is at the bottom and ties everything together. ● The logic in the click handler is straightforward: it reads input, updates a score based on several checks, and then updates the UI. ● Names for variables and functions are descriptive and consistent, so it’s easy to tell what they’re used for. Google Apps Script: ● The main functions are clearly named: exportEmailToSheet and parseForwardedBlock. ● There is a helpful comment block describing the expected forwarded message format and what the parser returns. ● The regex logic is a bit dense, but it’s all focused on one goal: extracting the original sender and body. CSS (style.css): ● The CSS is easy to follow and uses a simple card layout. ● Classes like .card, .score-circle, .progress, and .verdict-safe match their purposes. ● The score meter and verdict colors are visually clear. The main readability weakness is that some functions (especially the click handler in app.js and parseForwardedBlock in the script) do many things at once. Splitting them into smaller helper functions could make the code even easier to read and maintain. b. Code smells / issues and suggested improvements Here are some issues I noticed and how they could be improved: 1. Firebase Functions backend isn’t really used yet ○ The functions project sets some global options but doesn’t export any real Cloud Functions. ○ The front-end used to try to call a function named analyzeEmail, but that code is commented out and no such function exists. Suggestion: ○ Decide whether to actually use a backend function for analysis. If yes, implement and export it. If not, delete or clearly mark unused backend code to avoid confusion. 2. Possible duplicate data in Google Sheets ○ The Apps Script processes all threads with the label forwarded and appends rows to the sheet every time it runs. ○ The line that would remove the label after processing is commented out, so running the script again might re-add the same messages. Suggestion: ○ Either remove or change the Gmail label once a message is processed, or track message IDs in the sheet to avoid duplicates. 3. No safety checks for missing DOM elements ○ The script assumes elements like analyzeBtn, scoreCircle, and scoreBar always exist. ○ If the HTML structure changes or the script runs too early, this can cause errors. Suggestion: ○ Add simple checks to make sure elements exist before using them, or set up the code to run after DOMContentLoaded. 4. Hard-coded (“magic”) numbers in the scoring rules ○ The scoring logic is full of numbers like +15, +20, +25 scattered through the code. Suggestion: ○ Move these values into a single configuration object or set of constants at the top of the file. That makes it easier to tune the scoring later and understand the relative weight of each rule. 5. Styling is partly in JS, partly in CSS ○ Colors for the score bar and circle are set directly in JavaScript. Suggestion: ○ Put these colors into CSS classes (for example .score-safe, .score-suspicious, .score-high) and then add/remove classes in JS. That keeps visual design mostly in one place. 6. Garbage text at the end of style.css ○ The CSS file has corrupted-looking text and notes that really belong in the README. Suggestion: ○ Remove that extra text from the CSS file and put any instructions or notes into the README or documentation instead. c. Were you able to run the project? Does it work as expected? The project is intended to run as a simple static website: ● It can be served locally (e.g., with VS Code Live Server), or ● Viewed at the Firebase hosting URL: https://email-scanner-5ff74.web.app/. When used as intended, the flow is: 1. Paste an email’s body (and optionally headers) into the text areas. 2. Click the “Analyze” button. 3. See a score, a verdict, and a list explaining what the tool found. Based on reading the code, the behavior lines up with the project’s description: emails with more phishing-related patterns get higher scores and harsher verdicts, while normal-looking messages score low and are marked as “Likely Safe.” (If you actually tested it, you can optionally add a sentence like “When I tried a fake phishing email, it correctly flagged it as high risk.”) d. Bugs or potential issues; user-friendliness Potential issues: ● Firestore rules are currently very permissive until a certain date and then deny all requests. This is fine for a quick class project but would be unsafe and break things in a real app if Firestore gets used more heavily later. ● The garbage text in style.css could cause unexpected styling issues. ● There isn’t much visible error handling on the front-end if something goes wrong; the user might not know why a failure happened. User-friendliness: ● What works well: ○ The page is clean and not cluttered. ○ The score circle and progress bar are easy to read. ○ The breakdown list that explains “why” is very user-friendly. ○ The copy-to-clipboard button for the forward address is convenient. ● What could be improved: ○ The app uses alert() popups for copy messages, which can feel a bit old-school and disruptive. ○ A short intro or help section on the page explaining what the tool does and how to use it would be helpful. ○ Relying mainly on color for verdicts might not be great for color-blind users; icons or labels could make verdicts clearer. 4. Recommendations a. Three main areas for improvement 1. Clarify and finish the backend story ○ Decide if the project will use a Cloud Function for analysis; if yes, build it and hook it up. If not, remove unused backend code and just document that the project is front-end + Apps Script. 2. Tighten Firestore security (if used later) ○ Replace the temporary “open until this date” rule with more specific rules, especially if user data is ever stored. For example, allow access only to certain collections, and possibly only for authenticated users. 3. Avoid duplicate entries in the email-to-sheet workflow ○ Update the Apps Script so that emails aren’t written more than once, either by removing/changing labels after processing or by tracking which message IDs have already been stored. Optional extra areas: ● Clean up style.css and keep instructions in the README. ● Break up large functions and centralize the scoring weights into a config object. b. Suggested additional features and why they’d help 1. Scan history / mini dashboard ○ Let users see their recent scans, scores, and verdicts. This would make the app feel more like a tool they can use regularly instead of a one-off tester. 2. Help / “What this tool checks for” section ○ Briefly explain each rule (shorteners, punycode, mismatch, etc.) in non-technical language. This would help teach users what to look out for in their own inboxes. 3. Connecting the Apps Script data to the UI ○ Eventually, data collected in Sheets could be summarized or visualized in the web app (for example: number of high-risk emails detected). That would make the project feel more “full-stack.” 4. Adjustable sensitivity level ○ Add a simple control for “Strict,” “Normal,” or “Relaxed” mode that slightly changes the scoring thresholds or rule weights. This gives users more control over how aggressively the tool flags emails. 5. Overall Review a. What I learned from reviewing this code By reviewing this project, I learned how a relatively simple front-end plus a few smart rules can form the basis of a useful security tool. I also saw how Google Apps Script can be used to connect Gmail and Google Sheets in a practical way, without requiring a big backend system. It also reminded me how important it is to think about edge cases and real data: emails don’t come in neat formats, and there’s always going to be some weird formatting or unusual pattern to handle. Finally, it made me more aware of how useful it is to explain results to users rather than just giving them a number. b. Most impressive parts The parts I found most impressive were: ● The variety of realistic checks used to detect potential phishing emails. ● The way the app explains its decision with a breakdown list instead of just showing a score. ● The clever use of Gmail labels and Google Sheets to collect real examples for testing and analysis. 6. Feedback and Reflection After doing this code review and writing up my feedback, I realized how much you can learn just by carefully reading someone else’s project. Looking at this email scanner forced me to pay attention not only to whether the code “worked,” but also to how understandable it was, how secure it might be, and how easy it is for users to interact with. It made me think more about things like clear naming, avoiding duplicate data, and designing rules that are explainable to normal users, not just programmers. Writing feedback for another student also made me more aware of how to communicate suggestions respectfully and clearly. Instead of just saying “this is wrong,” I had to explain why something might be a problem (like open Firestore rules or possible duplicate data) and give a concrete idea of how to improve it. That’s a useful skill for teamwork and for future jobs, where giving and receiving feedback is a big part of working on real projects. Overall, reviewing this project gave me new ideas about how to structure my own code, how to think about security and user experience, and how to give feedback that’s specific, honest, and helpful. RYAN: Sean's peer review shows me where my project currently stands and how effectively it meets its goal of detecting phishing emails. He liked the clarity of my phishing checks and the educational breakdown of results, but showed me what parts need refinement (google Script). Sean has me thinking about architectural considerations, like how real-world email data is messy, how important clear explanations are for non-technical users, and where my Apps Script and backend setup could be strengthened. Overall, it will help me validate the direction of my project while giving me a roadmap of specific improvements. Based on this feedback, my next steps will focus on strengthening the backend, improving data handling, and polishing the user experience. I plan to finalize whether analysis should happen entirely in the browser or partly through Firebase Functions, and then clean up unused backend code to make the project more focused. I’ll also update the Google Apps Script to prevent duplicate entries and tighten security rules so the system is safer and more production-ready. On the front-end I need to refine the scoring logic.